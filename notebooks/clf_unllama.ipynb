{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "156b97d0-ed43-4148-8ddb-c7fe4756a417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"3\"\n",
    "TOKEN = \"\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7f41563-3055-4a65-a343-98c221b17e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d38387f296a4b0195452c5c3245f905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/9263 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd89ebf2a9c477998256c8c6c7387bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2619c7da575d4be3a088221ac06ee4ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2936 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1c35f92232645d19b540ef1100bc499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/8023 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers.models.llama.modeling_llama import *\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "path = \"so\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# train_set = load_dataset('json', data_files=os.path.join('so', 'data_train.json'))[\"train\"]\n",
    "# test_set = load_dataset('json', data_files=os.path.join('so', 'data_test.json'))[\"train\"]\n",
    "# dev_set = load_dataset('json', data_files=os.path.join('so', 'data_dev.json'))[\"train\"]\n",
    "\n",
    "dataset = load_dataset('json', data_files=os.path.join(path, 'data_train.json'), download_mode='force_redownload')\n",
    "dataset[\"test\"] = load_dataset('json', data_files=os.path.join(path, 'data_test.json'), download_mode='force_redownload')[\"train\"]\n",
    "dataset[\"validation\"] = load_dataset('json', data_files=os.path.join(path, 'data_dev.json'), download_mode='force_redownload')[\"train\"]\n",
    "dataset[\"gh\"] = load_dataset('json', data_files=os.path.join(path, 'data_gh.json'), download_mode='force_redownload')[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82cfb350",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {'O': 0, 'B-ALG': 1, 'I-ALG': 2, 'B-APP': 3, 'I-APP': 4, 'B-CB': 5, 'I-CB': 6, 'B-CLA': 7, 'I-CLA': 8, 'B-DEV': 9, 'I-DEV': 10, 'B-DS': 11, 'I-DS': 12, 'B-DT': 13, 'I-DT': 14, 'B-FN': 15, 'I-FN': 16, 'B-FT': 17, 'I-FT': 18, 'B-FUN': 19, 'I-FUN': 20, 'B-HXT': 21, 'I-HXT': 22, 'B-LAN': 23, 'I-LAN': 24, 'B-LIB': 25, 'I-LIB': 26, 'B-OS': 27, 'I-OS': 28, 'B-UIE': 29, 'I-UIE': 30, 'B-UN': 31, 'I-UN': 32, 'B-VAL': 33, 'I-VAL': 34, 'B-VAR': 35, 'I-VAR': 36, 'B-VER': 37, 'I-VER': 38, 'B-WEB': 39, 'I-WEB': 40}\n",
    "id2label = {label2id[x]: x for x in label2id}\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 8\n",
    "learning_rate = 1e-4\n",
    "max_length = 64\n",
    "lora_r = 12\n",
    "\n",
    "\n",
    "\n",
    "_CONFIG_FOR_DOC = \"LlamaConfig\"\n",
    "\n",
    "\n",
    "# Copied from transformers.models.bart.modeling_bart._make_causal_mask\n",
    "def _make_causal_mask(\n",
    "    input_ids_shape: torch.Size, dtype: torch.dtype, device: torch.device, past_key_values_length: int = 0\n",
    "):\n",
    "    \"\"\"\n",
    "    Make causal mask used for bi-directional self-attention.\n",
    "    \"\"\"\n",
    "    bsz, tgt_len = input_ids_shape\n",
    "    mask = torch.full((tgt_len, tgt_len), torch.finfo(dtype).min, device=device)\n",
    "    mask_cond = torch.arange(mask.size(-1), device=device)\n",
    "    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
    "    mask = mask.to(dtype)\n",
    "\n",
    "    if past_key_values_length > 0:\n",
    "        mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype, device=device), mask], dim=-1)\n",
    "    return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\n",
    "\n",
    "\n",
    "# Copied from transformers.models.bart.modeling_bart._expand_mask\n",
    "def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n",
    "    \"\"\"\n",
    "    bsz, src_len = mask.size()\n",
    "    tgt_len = tgt_len if tgt_len is not None else src_len\n",
    "\n",
    "    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n",
    "\n",
    "    inverted_mask = 1.0 - expanded_mask\n",
    "\n",
    "    return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n",
    "\n",
    "\n",
    "class UnmaskingLlamaModel(LlamaPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n",
    "\n",
    "    Args:\n",
    "        config: LlamaConfig\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        super().__init__(config)\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
    "        self.layers = nn.ModuleList([LlamaDecoderLayer(config, idx) for idx in range(config.num_hidden_layers)])\n",
    "        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embed_tokens = value\n",
    "\n",
    "    # Copied from transformers.models.bart.modeling_bart.BartDecoder._prepare_decoder_attention_mask\n",
    "    def _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_embeds, past_key_values_length):\n",
    "        # create causal mask\n",
    "        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "        combined_attention_mask = None\n",
    "        if input_shape[-1] > 1:\n",
    "            combined_attention_mask = _make_causal_mask(\n",
    "                input_shape,\n",
    "                inputs_embeds.dtype,\n",
    "                device=inputs_embeds.device,\n",
    "                past_key_values_length=past_key_values_length,\n",
    "            )\n",
    "        if attention_mask is not None:\n",
    "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "            expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]).to(\n",
    "                inputs_embeds.device\n",
    "            )\n",
    "            combined_attention_mask = (\n",
    "                expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask\n",
    "            )\n",
    "\n",
    "        return combined_attention_mask\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # retrieve input_ids and inputs_embeds\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            batch_size, seq_length = input_ids.shape\n",
    "        elif inputs_embeds is not None:\n",
    "            batch_size, seq_length, _ = inputs_embeds.shape\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n",
    "\n",
    "        seq_length_with_past = seq_length\n",
    "        past_key_values_length = 0\n",
    "\n",
    "        if past_key_values is not None:\n",
    "            past_key_values_length = past_key_values[0][0].shape[2]\n",
    "            seq_length_with_past = seq_length_with_past + past_key_values_length\n",
    "\n",
    "        if position_ids is None:\n",
    "            device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "            position_ids = torch.arange(\n",
    "                past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device\n",
    "            )\n",
    "            position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\n",
    "        else:\n",
    "            position_ids = position_ids.view(-1, seq_length).long()\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embed_tokens(input_ids)\n",
    "        # embed positions\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(\n",
    "                (batch_size, seq_length_with_past), dtype=torch.bool, device=inputs_embeds.device\n",
    "            )\n",
    "        # causal mask\n",
    "        '''\n",
    "        attention_mask = self._prepare_decoder_attention_mask(\n",
    "            attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length\n",
    "        )\n",
    "        print('unmasking attention mask:')\n",
    "        print(attention_mask)\n",
    "        '''\n",
    "        # remove causal mask\n",
    "        attention_mask = torch.zeros(\n",
    "            (batch_size, 1, seq_length, seq_length), device=inputs_embeds.device\n",
    "        )\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        if self.gradient_checkpointing and self.training:\n",
    "            if use_cache:\n",
    "                logger.warning_once(\n",
    "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                )\n",
    "                use_cache = False\n",
    "\n",
    "        # decoder layers\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attns = () if output_attentions else None\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "\n",
    "        for idx, decoder_layer in enumerate(self.layers):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "\n",
    "            past_key_value = past_key_values[idx] if past_key_values is not None else None\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        # None for past_key_value\n",
    "                        return module(*inputs, past_key_value, output_attentions)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(decoder_layer),\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    position_ids,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = decoder_layer(\n",
    "                    hidden_states,\n",
    "                    attention_mask=attention_mask,\n",
    "                    position_ids=position_ids,\n",
    "                    past_key_value=past_key_value,\n",
    "                    output_attentions=output_attentions,\n",
    "                    use_cache=use_cache,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attns += (layer_outputs[1],)\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        # add hidden states from the last decoder layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        next_cache = next_decoder_cache if use_cache else None\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n",
    "        return BaseModelOutputWithPast(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attns,\n",
    "        )\n",
    "\n",
    "class UnmaskingLlamaForTokenClassification(LlamaPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.model = UnmaskingLlamaModel(config)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.model.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.model.embed_tokens = value\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69a06ca0-0f4f-46e9-8ccd-bdc921980aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6d1fdda613432aaa8f93cf139263d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5400763715c42f69457198e36309877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64213310f9324d62a73fbe642388e1cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aff0417858a24b1fb05bb113efc296e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of UnmaskingLlamaForTokenClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28cf9db89ae549c192b69383b94c5381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6ebaef6de846ad9740a388d2b3df84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f3c74bbf704db78bb4c31316796463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d84ba002125647478cce9174a5fbb1d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "#base_model = \"codellama/CodeLlama-7b-hf\"\n",
    "base_model = \"meta-llama/Llama-2-7b-hf\"\n",
    "model = UnmaskingLlamaForTokenClassification.from_pretrained(\n",
    "    base_model, num_labels=len(label2id), id2label=id2label, label2id=label2id, token=TOKEN\n",
    ").to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, token=TOKEN)\n",
    "peft_config = LoraConfig(task_type=TaskType.TOKEN_CLS, inference_mode=False, r=lora_r, lora_alpha=32, lora_dropout=0.1)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27bb5160-3363-4337-bf4c-5224e37d5e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d07ed42fcf41b082d1b47a5540f71a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9263 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc3012a3cc694a7598339d34576272b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e5afaa805964a9e908fad77c6b57057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2936 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa3134778434e5281106a2298dc7fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8023 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "max_length = 64\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], is_split_into_words=True, padding='longest', max_length=max_length, truncation=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "tokenized_ds = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4814bdc-f0e4-447d-9c70-164e21330bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "label_list = list(label2id.keys())\n",
    "\n",
    "def compute_metrics(p, full=False):\n",
    "    predictions, labels = p\n",
    "    if full is False:\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    if full:\n",
    "        return results\n",
    "    else:\n",
    "        return {\n",
    "            \"precision\": results[\"overall_precision\"],\n",
    "            \"recall\": results[\"overall_recall\"],\n",
    "            \"f1\": results[\"overall_f1\"],\n",
    "            \"accuracy\": results[\"overall_accuracy\"],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64c32a48-d134-4445-ace3-84d1ff2d4067",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/root/anaconda3/envs/codeGen/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11580' max='11580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11580/11580 3:19:05, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.495600</td>\n",
       "      <td>0.480457</td>\n",
       "      <td>0.204696</td>\n",
       "      <td>0.075960</td>\n",
       "      <td>0.110802</td>\n",
       "      <td>0.896072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.446300</td>\n",
       "      <td>0.437991</td>\n",
       "      <td>0.312446</td>\n",
       "      <td>0.099102</td>\n",
       "      <td>0.150475</td>\n",
       "      <td>0.900434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.395400</td>\n",
       "      <td>0.413238</td>\n",
       "      <td>0.258758</td>\n",
       "      <td>0.114620</td>\n",
       "      <td>0.158868</td>\n",
       "      <td>0.902870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.358700</td>\n",
       "      <td>0.393299</td>\n",
       "      <td>0.290456</td>\n",
       "      <td>0.209638</td>\n",
       "      <td>0.243517</td>\n",
       "      <td>0.906397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.323800</td>\n",
       "      <td>0.369737</td>\n",
       "      <td>0.370663</td>\n",
       "      <td>0.229785</td>\n",
       "      <td>0.283697</td>\n",
       "      <td>0.911734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.298600</td>\n",
       "      <td>0.344761</td>\n",
       "      <td>0.379159</td>\n",
       "      <td>0.282331</td>\n",
       "      <td>0.323658</td>\n",
       "      <td>0.916282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.278300</td>\n",
       "      <td>0.338777</td>\n",
       "      <td>0.407654</td>\n",
       "      <td>0.316090</td>\n",
       "      <td>0.356080</td>\n",
       "      <td>0.917651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.251500</td>\n",
       "      <td>0.336804</td>\n",
       "      <td>0.471366</td>\n",
       "      <td>0.320447</td>\n",
       "      <td>0.381524</td>\n",
       "      <td>0.921410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.247300</td>\n",
       "      <td>0.316984</td>\n",
       "      <td>0.457810</td>\n",
       "      <td>0.357473</td>\n",
       "      <td>0.401468</td>\n",
       "      <td>0.923382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.228800</td>\n",
       "      <td>0.316162</td>\n",
       "      <td>0.451143</td>\n",
       "      <td>0.370814</td>\n",
       "      <td>0.407053</td>\n",
       "      <td>0.923800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/codeGen/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/codeGen/lib/python3.9/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6606b061-257de8fe090308c3038c1ee9;63dd0d38-b857-4ec9-980d-8f7dfd730710)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "Repo model meta-llama/Llama-2-7b-hf is gated. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/codeGen/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/codeGen/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/codeGen/lib/python3.9/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6606b506-4a97e676410f2b837db55712;d9c61634-6b9c-4a50-9162-d0d16c0c4e5c)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "Repo model meta-llama/Llama-2-7b-hf is gated. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/codeGen/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/codeGen/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/codeGen/lib/python3.9/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6606b9b4-27c839407ba67cee674c93e5;7bc9aa0e-be59-4da2-a4ce-96f1b8f8f0b5)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "Repo model meta-llama/Llama-2-7b-hf is gated. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/codeGen/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/codeGen/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/codeGen/lib/python3.9/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6606be61-609544645660797957931f56;ae800d07-ef82-4ccd-b3e6-d68aaf79a2a1)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "Repo model meta-llama/Llama-2-7b-hf is gated. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/codeGen/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/codeGen/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/codeGen/lib/python3.9/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6606c30e-3c898a92423192da5237502c;f84519b9-3aca-4e53-8358-fff14c61bd98)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "Repo model meta-llama/Llama-2-7b-hf is gated. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/codeGen/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/codeGen/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/codeGen/lib/python3.9/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6606c7b5-7a275a0664d37aa666024b7f;2a53d167-e7d8-47e6-8445-8b52d43f37c6)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "Repo model meta-llama/Llama-2-7b-hf is gated. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/codeGen/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/codeGen/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/codeGen/lib/python3.9/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6606cc61-777819355af6ffb7724af414;2cc6b678-3c4b-4343-93d6-96f1c65ca2eb)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "Repo model meta-llama/Llama-2-7b-hf is gated. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/codeGen/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/codeGen/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/codeGen/lib/python3.9/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6606d10c-43a8a50464a8bd770e16d844;715071e4-5f1a-4a9e-b8bc-56d9394ae139)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "Repo model meta-llama/Llama-2-7b-hf is gated. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/codeGen/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/codeGen/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/codeGen/lib/python3.9/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6606d5b9-1246096555e7e4290660cf88;7b8c1e70-e77c-4850-8b48-1e0af512b0fa)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "Repo model meta-llama/Llama-2-7b-hf is gated. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/codeGen/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/codeGen/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/codeGen/lib/python3.9/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6606da61-338221d6334b2219474ebe22;f95a0d08-b1e0-48b5-80dd-5c2000bee579)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "Repo model meta-llama/Llama-2-7b-hf is gated. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/codeGen/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=11580, training_loss=0.33638687611449775, metrics={'train_runtime': 11948.1898, 'train_samples_per_second': 7.753, 'train_steps_per_second': 0.969, 'total_flos': 2.3059619972289792e+17, 'train_loss': 0.33638687611449775, 'epoch': 10.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 8\n",
    "learning_rate = 1e-4\n",
    "\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"llama-more\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45d34533-33d8-4a6d-93d5-2a6ddcca6620",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"unllama.txt\", \"w\") as f:\n",
    "    f.write(f\"{trainer.state.log_history}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4de024fc-7c1f-4972-b99a-a9532c492da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/codeGen/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/codeGen/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "a = trainer.predict(tokenized_ds[\"validation\"])\n",
    "b = trainer.predict(tokenized_ds[\"test\"])\n",
    "c = trainer.predict(tokenized_ds[\"gh\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc69a355-86e3-479b-91fd-8734d308b77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ALG': {'f1': 0.0, 'number': 9, 'precision': 0.0, 'recall': 0.0},\n",
      " 'APP': {'f1': 0.4224364592462752,\n",
      "         'number': 480,\n",
      "         'precision': 0.36459909228441756,\n",
      "         'recall': 0.5020833333333333},\n",
      " 'CB': {'f1': 0.17509727626459143,\n",
      "        'number': 244,\n",
      "        'precision': 0.16666666666666666,\n",
      "        'recall': 0.18442622950819673},\n",
      " 'CLA': {'f1': 0.4277456647398844,\n",
      "         'number': 406,\n",
      "         'precision': 0.40305010893246185,\n",
      "         'recall': 0.45566502463054187},\n",
      " 'DEV': {'f1': 0.1935483870967742,\n",
      "         'number': 149,\n",
      "         'precision': 0.4864864864864865,\n",
      "         'recall': 0.12080536912751678},\n",
      " 'DS': {'f1': 0.672316384180791,\n",
      "        'number': 177,\n",
      "        'precision': 0.672316384180791,\n",
      "        'recall': 0.672316384180791},\n",
      " 'DT': {'f1': 0.6037735849056602,\n",
      "        'number': 134,\n",
      "        'precision': 0.8205128205128205,\n",
      "        'recall': 0.47761194029850745},\n",
      " 'FN': {'f1': 0.4170616113744075,\n",
      "        'number': 134,\n",
      "        'precision': 0.5714285714285714,\n",
      "        'recall': 0.3283582089552239},\n",
      " 'FT': {'f1': 0.40625,\n",
      "        'number': 90,\n",
      "        'precision': 0.6842105263157895,\n",
      "        'recall': 0.28888888888888886},\n",
      " 'FUN': {'f1': 0.3198090692124105,\n",
      "         'number': 261,\n",
      "         'precision': 0.4240506329113924,\n",
      "         'recall': 0.2567049808429119},\n",
      " 'HXT': {'f1': 0.5901639344262294,\n",
      "         'number': 38,\n",
      "         'precision': 0.782608695652174,\n",
      "         'recall': 0.47368421052631576},\n",
      " 'LAN': {'f1': 0.6925207756232686,\n",
      "         'number': 154,\n",
      "         'precision': 0.6038647342995169,\n",
      "         'recall': 0.8116883116883117},\n",
      " 'LIB': {'f1': 0.34183673469387754,\n",
      "         'number': 226,\n",
      "         'precision': 0.4036144578313253,\n",
      "         'recall': 0.29646017699115046},\n",
      " 'OS': {'f1': 0.5740740740740741,\n",
      "        'number': 73,\n",
      "        'precision': 0.8857142857142857,\n",
      "        'recall': 0.4246575342465753},\n",
      " 'UIE': {'f1': 0.4851485148514852,\n",
      "         'number': 330,\n",
      "         'precision': 0.532608695652174,\n",
      "         'recall': 0.44545454545454544},\n",
      " 'UN': {'f1': 0.27906976744186046,\n",
      "        'number': 30,\n",
      "        'precision': 0.46153846153846156,\n",
      "        'recall': 0.2},\n",
      " 'VAL': {'f1': 0.25542168674698795,\n",
      "         'number': 269,\n",
      "         'precision': 0.363013698630137,\n",
      "         'recall': 0.1970260223048327},\n",
      " 'VAR': {'f1': 0.10026385224274405,\n",
      "         'number': 293,\n",
      "         'precision': 0.22093023255813954,\n",
      "         'recall': 0.06484641638225255},\n",
      " 'VER': {'f1': 0.6887966804979254,\n",
      "         'number': 137,\n",
      "         'precision': 0.7980769230769231,\n",
      "         'recall': 0.6058394160583942},\n",
      " 'WEB': {'f1': 0.1702127659574468,\n",
      "         'number': 39,\n",
      "         'precision': 0.5,\n",
      "         'recall': 0.10256410256410256},\n",
      " 'overall_accuracy': 0.9237998004501473,\n",
      " 'overall_f1': 0.40705319784817684,\n",
      " 'overall_precision': 0.4511427625041404,\n",
      " 'overall_recall': 0.37081404846174787}\n",
      "{'ALG': {'f1': 0.0, 'number': 16, 'precision': 0.0, 'recall': 0.0},\n",
      " 'APP': {'f1': 0.4142715559960357,\n",
      "         'number': 407,\n",
      "         'precision': 0.34717607973421927,\n",
      "         'recall': 0.5135135135135135},\n",
      " 'CB': {'f1': 0.10922787193973635,\n",
      "        'number': 294,\n",
      "        'precision': 0.12236286919831224,\n",
      "        'recall': 0.09863945578231292},\n",
      " 'CLA': {'f1': 0.4100502512562814,\n",
      "         'number': 509,\n",
      "         'precision': 0.41975308641975306,\n",
      "         'recall': 0.40078585461689586},\n",
      " 'DEV': {'f1': 0.32,\n",
      "         'number': 53,\n",
      "         'precision': 0.5454545454545454,\n",
      "         'recall': 0.22641509433962265},\n",
      " 'DS': {'f1': 0.7494908350305499,\n",
      "        'number': 244,\n",
      "        'precision': 0.7449392712550608,\n",
      "        'recall': 0.7540983606557377},\n",
      " 'DT': {'f1': 0.6601941747572815,\n",
      "        'number': 111,\n",
      "        'precision': 0.7157894736842105,\n",
      "        'recall': 0.6126126126126126},\n",
      " 'FN': {'f1': 0.3816793893129771,\n",
      "        'number': 163,\n",
      "        'precision': 0.5050505050505051,\n",
      "        'recall': 0.3067484662576687},\n",
      " 'FT': {'f1': 0.3391812865497076,\n",
      "        'number': 127,\n",
      "        'precision': 0.6590909090909091,\n",
      "        'recall': 0.2283464566929134},\n",
      " 'FUN': {'f1': 0.33920704845814986,\n",
      "         'number': 266,\n",
      "         'precision': 0.4095744680851064,\n",
      "         'recall': 0.2894736842105263},\n",
      " 'HXT': {'f1': 0.1875,\n",
      "         'number': 52,\n",
      "         'precision': 0.5,\n",
      "         'recall': 0.11538461538461539},\n",
      " 'LAN': {'f1': 0.6327077747989276,\n",
      "         'number': 178,\n",
      "         'precision': 0.6051282051282051,\n",
      "         'recall': 0.6629213483146067},\n",
      " 'LIB': {'f1': 0.2985781990521327,\n",
      "         'number': 257,\n",
      "         'precision': 0.38181818181818183,\n",
      "         'recall': 0.245136186770428},\n",
      " 'OS': {'f1': 0.5423728813559322,\n",
      "        'number': 66,\n",
      "        'precision': 0.6153846153846154,\n",
      "        'recall': 0.48484848484848486},\n",
      " 'UIE': {'f1': 0.48126801152737747,\n",
      "         'number': 355,\n",
      "         'precision': 0.49262536873156343,\n",
      "         'recall': 0.4704225352112676},\n",
      " 'UN': {'f1': 0.18181818181818182,\n",
      "        'number': 24,\n",
      "        'precision': 0.3333333333333333,\n",
      "        'recall': 0.125},\n",
      " 'VAL': {'f1': 0.24550898203592814,\n",
      "         'number': 213,\n",
      "         'precision': 0.33884297520661155,\n",
      "         'recall': 0.19248826291079812},\n",
      " 'VAR': {'f1': 0.17120622568093385,\n",
      "         'number': 375,\n",
      "         'precision': 0.31654676258992803,\n",
      "         'recall': 0.11733333333333333},\n",
      " 'VER': {'f1': 0.6494845360824741,\n",
      "         'number': 111,\n",
      "         'precision': 0.7590361445783133,\n",
      "         'recall': 0.5675675675675675},\n",
      " 'WEB': {'f1': 0.0, 'number': 39, 'precision': 0.0, 'recall': 0.0},\n",
      " 'overall_accuracy': 0.9247039103680995,\n",
      " 'overall_f1': 0.4,\n",
      " 'overall_precision': 0.4462519936204147,\n",
      " 'overall_recall': 0.36243523316062176}\n",
      "{'ALG': {'f1': 0.0, 'number': 53, 'precision': 0.0, 'recall': 0.0},\n",
      " 'APP': {'f1': 0.3515059092642013,\n",
      "         'number': 1224,\n",
      "         'precision': 0.3295210864903502,\n",
      "         'recall': 0.37663398692810457},\n",
      " 'CB': {'f1': 0.023898431665421955,\n",
      "        'number': 555,\n",
      "        'precision': 0.02040816326530612,\n",
      "        'recall': 0.02882882882882883},\n",
      " 'CLA': {'f1': 0.153987167736022,\n",
      "         'number': 274,\n",
      "         'precision': 0.10281517747858017,\n",
      "         'recall': 0.30656934306569344},\n",
      " 'DEV': {'f1': 0.23923444976076555,\n",
      "         'number': 153,\n",
      "         'precision': 0.44642857142857145,\n",
      "         'recall': 0.16339869281045752},\n",
      " 'DS': {'f1': 0.4968553459119497,\n",
      "        'number': 156,\n",
      "        'precision': 0.4876543209876543,\n",
      "        'recall': 0.5064102564102564},\n",
      " 'DT': {'f1': 0.5277777777777777,\n",
      "        'number': 85,\n",
      "        'precision': 0.6440677966101694,\n",
      "        'recall': 0.4470588235294118},\n",
      " 'FN': {'f1': 0.12063492063492065,\n",
      "        'number': 227,\n",
      "        'precision': 0.07938718662952646,\n",
      "        'recall': 0.2511013215859031},\n",
      " 'FT': {'f1': 0.4423076923076923,\n",
      "        'number': 138,\n",
      "        'precision': 0.6571428571428571,\n",
      "        'recall': 0.3333333333333333},\n",
      " 'FUN': {'f1': 0.20046620046620045,\n",
      "         'number': 143,\n",
      "         'precision': 0.15034965034965034,\n",
      "         'recall': 0.3006993006993007},\n",
      " 'HXT': {'f1': 0.25,\n",
      "         'number': 13,\n",
      "         'precision': 0.6666666666666666,\n",
      "         'recall': 0.15384615384615385},\n",
      " 'LAN': {'f1': 0.5778688524590163,\n",
      "         'number': 209,\n",
      "         'precision': 0.5053763440860215,\n",
      "         'recall': 0.6746411483253588},\n",
      " 'LIB': {'f1': 0.20444444444444443,\n",
      "         'number': 579,\n",
      "         'precision': 0.2866043613707165,\n",
      "         'recall': 0.15889464594127806},\n",
      " 'OS': {'f1': 0.5378787878787878,\n",
      "        'number': 175,\n",
      "        'precision': 0.797752808988764,\n",
      "        'recall': 0.4057142857142857},\n",
      " 'UIE': {'f1': 0.54983922829582,\n",
      "         'number': 282,\n",
      "         'precision': 0.5029411764705882,\n",
      "         'recall': 0.6063829787234043},\n",
      " 'UN': {'f1': 0.1830065359477124,\n",
      "        'number': 182,\n",
      "        'precision': 0.22580645161290322,\n",
      "        'recall': 0.15384615384615385},\n",
      " 'VAL': {'f1': 0.0, 'number': 0, 'precision': 0.0, 'recall': 0.0},\n",
      " 'VAR': {'f1': 0.04850746268656717,\n",
      "         'number': 348,\n",
      "         'precision': 0.06914893617021277,\n",
      "         'recall': 0.03735632183908046},\n",
      " 'VER': {'f1': 0.5996275605214152,\n",
      "         'number': 333,\n",
      "         'precision': 0.7892156862745098,\n",
      "         'recall': 0.48348348348348347},\n",
      " 'WEB': {'f1': 0.17177914110429449,\n",
      "         'number': 145,\n",
      "         'precision': 0.7777777777777778,\n",
      "         'recall': 0.09655172413793103},\n",
      " 'overall_accuracy': 0.8942130984630883,\n",
      " 'overall_f1': 0.27140719880313297,\n",
      " 'overall_precision': 0.25324355394974546,\n",
      " 'overall_recall': 0.2923777019340159}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pprint import pprint\n",
    "\n",
    "def get_dct(x, text):\n",
    "    pred = np.argmax(x.predictions, axis=2)\n",
    "    dct = compute_metrics((pred, tokenized_ds[text][\"labels\"]), True)\n",
    "    with open(f'unllama_{text}.pickle', 'wb') as f:\n",
    "        pickle.dump(dct, f)\n",
    "\n",
    "    return dct\n",
    "\n",
    "pprint(get_dct(a, \"validation\"))\n",
    "pprint(get_dct(b, \"test\"))\n",
    "pprint(get_dct(c, \"gh\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9265d2-d330-4794-bef6-5ad52f4c88fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codeGen",
   "language": "python",
   "name": "codegen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

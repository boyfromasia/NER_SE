{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "156b97d0-ed43-4148-8ddb-c7fe4756a417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"2\"\n",
    "TOKEN = \"\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7f41563-3055-4a65-a343-98c221b17e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4109cd343664459a35c9f4d4b2df063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/9263 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9440e44ccd024865b25fbf08d64fd377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a36ebe0f1a4621bb4d502ea33b9326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2936 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76255460436043ac97145ea525601131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/8023 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers.models.llama.modeling_llama import *\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "path = \"so\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# train_set = load_dataset('json', data_files=os.path.join('so', 'data_train.json'))[\"train\"]\n",
    "# test_set = load_dataset('json', data_files=os.path.join('so', 'data_test.json'))[\"train\"]\n",
    "# dev_set = load_dataset('json', data_files=os.path.join('so', 'data_dev.json'))[\"train\"]\n",
    "\n",
    "dataset = load_dataset('json', data_files=os.path.join(path, 'data_train.json'), download_mode='force_redownload')\n",
    "dataset[\"test\"] = load_dataset('json', data_files=os.path.join(path, 'data_test.json'), download_mode='force_redownload')[\"train\"]\n",
    "dataset[\"validation\"] = load_dataset('json', data_files=os.path.join(path, 'data_dev.json'), download_mode='force_redownload')[\"train\"]\n",
    "dataset[\"gh\"] = load_dataset('json', data_files=os.path.join(path, 'data_gh.json'), download_mode='force_redownload')[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82cfb350",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {'O': 0, 'B-ALG': 1, 'I-ALG': 2, 'B-APP': 3, 'I-APP': 4, 'B-CB': 5, 'I-CB': 6, 'B-CLA': 7, 'I-CLA': 8, 'B-DEV': 9, 'I-DEV': 10, 'B-DS': 11, 'I-DS': 12, 'B-DT': 13, 'I-DT': 14, 'B-FN': 15, 'I-FN': 16, 'B-FT': 17, 'I-FT': 18, 'B-FUN': 19, 'I-FUN': 20, 'B-HXT': 21, 'I-HXT': 22, 'B-LAN': 23, 'I-LAN': 24, 'B-LIB': 25, 'I-LIB': 26, 'B-OS': 27, 'I-OS': 28, 'B-UIE': 29, 'I-UIE': 30, 'B-UN': 31, 'I-UN': 32, 'B-VAL': 33, 'I-VAL': 34, 'B-VAR': 35, 'I-VAR': 36, 'B-VER': 37, 'I-VER': 38, 'B-WEB': 39, 'I-WEB': 40}\n",
    "id2label = {label2id[x]: x for x in label2id}\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 8\n",
    "learning_rate = 1e-4\n",
    "max_length = 64\n",
    "lora_r = 12\n",
    "\n",
    "\n",
    "\n",
    "_CONFIG_FOR_DOC = \"LlamaConfig\"\n",
    "\n",
    "\n",
    "# Copied from transformers.models.bart.modeling_bart._make_causal_mask\n",
    "def _make_causal_mask(\n",
    "    input_ids_shape: torch.Size, dtype: torch.dtype, device: torch.device, past_key_values_length: int = 0\n",
    "):\n",
    "    \"\"\"\n",
    "    Make causal mask used for bi-directional self-attention.\n",
    "    \"\"\"\n",
    "    bsz, tgt_len = input_ids_shape\n",
    "    mask = torch.full((tgt_len, tgt_len), torch.finfo(dtype).min, device=device)\n",
    "    mask_cond = torch.arange(mask.size(-1), device=device)\n",
    "    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
    "    mask = mask.to(dtype)\n",
    "\n",
    "    if past_key_values_length > 0:\n",
    "        mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype, device=device), mask], dim=-1)\n",
    "    return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\n",
    "\n",
    "\n",
    "# Copied from transformers.models.bart.modeling_bart._expand_mask\n",
    "def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n",
    "    \"\"\"\n",
    "    bsz, src_len = mask.size()\n",
    "    tgt_len = tgt_len if tgt_len is not None else src_len\n",
    "\n",
    "    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n",
    "\n",
    "    inverted_mask = 1.0 - expanded_mask\n",
    "\n",
    "    return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n",
    "\n",
    "\n",
    "class UnmaskingLlamaModel(LlamaPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n",
    "\n",
    "    Args:\n",
    "        config: LlamaConfig\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        super().__init__(config)\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
    "        self.layers = nn.ModuleList([LlamaDecoderLayer(config, idx) for idx in range(config.num_hidden_layers)])\n",
    "        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embed_tokens = value\n",
    "\n",
    "    # Copied from transformers.models.bart.modeling_bart.BartDecoder._prepare_decoder_attention_mask\n",
    "    def _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_embeds, past_key_values_length):\n",
    "        # create causal mask\n",
    "        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "        combined_attention_mask = None\n",
    "        if input_shape[-1] > 1:\n",
    "            combined_attention_mask = _make_causal_mask(\n",
    "                input_shape,\n",
    "                inputs_embeds.dtype,\n",
    "                device=inputs_embeds.device,\n",
    "                past_key_values_length=past_key_values_length,\n",
    "            )\n",
    "        if attention_mask is not None:\n",
    "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "            expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]).to(\n",
    "                inputs_embeds.device\n",
    "            )\n",
    "            combined_attention_mask = (\n",
    "                expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask\n",
    "            )\n",
    "\n",
    "        return combined_attention_mask\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # retrieve input_ids and inputs_embeds\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            batch_size, seq_length = input_ids.shape\n",
    "        elif inputs_embeds is not None:\n",
    "            batch_size, seq_length, _ = inputs_embeds.shape\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n",
    "\n",
    "        seq_length_with_past = seq_length\n",
    "        past_key_values_length = 0\n",
    "\n",
    "        if past_key_values is not None:\n",
    "            past_key_values_length = past_key_values[0][0].shape[2]\n",
    "            seq_length_with_past = seq_length_with_past + past_key_values_length\n",
    "\n",
    "        if position_ids is None:\n",
    "            device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "            position_ids = torch.arange(\n",
    "                past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device\n",
    "            )\n",
    "            position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\n",
    "        else:\n",
    "            position_ids = position_ids.view(-1, seq_length).long()\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embed_tokens(input_ids)\n",
    "        # embed positions\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(\n",
    "                (batch_size, seq_length_with_past), dtype=torch.bool, device=inputs_embeds.device\n",
    "            )\n",
    "        # causal mask\n",
    "        '''\n",
    "        attention_mask = self._prepare_decoder_attention_mask(\n",
    "            attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length\n",
    "        )\n",
    "        print('unmasking attention mask:')\n",
    "        print(attention_mask)\n",
    "        '''\n",
    "        # remove causal mask\n",
    "        attention_mask = torch.zeros(\n",
    "            (batch_size, 1, seq_length, seq_length), device=inputs_embeds.device\n",
    "        )\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        if self.gradient_checkpointing and self.training:\n",
    "            if use_cache:\n",
    "                logger.warning_once(\n",
    "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                )\n",
    "                use_cache = False\n",
    "\n",
    "        # decoder layers\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attns = () if output_attentions else None\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "\n",
    "        for idx, decoder_layer in enumerate(self.layers):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "\n",
    "            past_key_value = past_key_values[idx] if past_key_values is not None else None\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        # None for past_key_value\n",
    "                        return module(*inputs, past_key_value, output_attentions)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(decoder_layer),\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    position_ids,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = decoder_layer(\n",
    "                    hidden_states,\n",
    "                    attention_mask=attention_mask,\n",
    "                    position_ids=position_ids,\n",
    "                    past_key_value=past_key_value,\n",
    "                    output_attentions=output_attentions,\n",
    "                    use_cache=use_cache,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attns += (layer_outputs[1],)\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        # add hidden states from the last decoder layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        next_cache = next_decoder_cache if use_cache else None\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n",
    "        return BaseModelOutputWithPast(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attns,\n",
    "        )\n",
    "\n",
    "class UnmaskingLlamaForTokenClassification(LlamaPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.model = UnmaskingLlamaModel(config)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.model.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.model.embed_tokens = value\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69a06ca0-0f4f-46e9-8ccd-bdc921980aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab6dd737583e4fb18038a48913cca2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of UnmaskingLlamaForTokenClassification were not initialized from the model checkpoint at codellama/CodeLlama-7b-hf and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "base_model = \"codellama/CodeLlama-7b-hf\"\n",
    "model = UnmaskingLlamaForTokenClassification.from_pretrained(\n",
    "    base_model, num_labels=len(label2id), id2label=id2label, label2id=label2id, token=TOKEN\n",
    ").to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, token=TOKEN)\n",
    "peft_config = LoraConfig(task_type=TaskType.TOKEN_CLS, inference_mode=False, r=lora_r, lora_alpha=32, lora_dropout=0.1)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27bb5160-3363-4337-bf4c-5224e37d5e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b609608a1bc348c291b9517995c28894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9263 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84272ce088614915a6d186e73a5fcedf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dad7c9383334c6ca735ba339acaeb09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2936 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8085e14087149888ee116cc78c5c078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8023 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "max_length = 64\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], is_split_into_words=True, padding='longest', max_length=max_length, truncation=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "tokenized_ds = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4814bdc-f0e4-447d-9c70-164e21330bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "label_list = list(label2id.keys())\n",
    "\n",
    "def compute_metrics(p, full=False):\n",
    "    predictions, labels = p\n",
    "    if full is False:\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    if full:\n",
    "        return results\n",
    "    else:\n",
    "        return {\n",
    "            \"precision\": results[\"overall_precision\"],\n",
    "            \"recall\": results[\"overall_recall\"],\n",
    "            \"f1\": results[\"overall_f1\"],\n",
    "            \"accuracy\": results[\"overall_accuracy\"],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64c32a48-d134-4445-ace3-84d1ff2d4067",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/root/anaconda3/envs/codeGen/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11580' max='11580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11580/11580 3:20:32, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.412700</td>\n",
       "      <td>0.325265</td>\n",
       "      <td>0.487750</td>\n",
       "      <td>0.352301</td>\n",
       "      <td>0.409105</td>\n",
       "      <td>0.920226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.217700</td>\n",
       "      <td>0.180074</td>\n",
       "      <td>0.653891</td>\n",
       "      <td>0.613123</td>\n",
       "      <td>0.632851</td>\n",
       "      <td>0.950066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.136000</td>\n",
       "      <td>0.164785</td>\n",
       "      <td>0.702014</td>\n",
       "      <td>0.645249</td>\n",
       "      <td>0.672436</td>\n",
       "      <td>0.955055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.090700</td>\n",
       "      <td>0.161704</td>\n",
       "      <td>0.669563</td>\n",
       "      <td>0.671386</td>\n",
       "      <td>0.670473</td>\n",
       "      <td>0.956029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.054400</td>\n",
       "      <td>0.188793</td>\n",
       "      <td>0.678236</td>\n",
       "      <td>0.649061</td>\n",
       "      <td>0.663328</td>\n",
       "      <td>0.955310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.033200</td>\n",
       "      <td>0.191364</td>\n",
       "      <td>0.666933</td>\n",
       "      <td>0.681459</td>\n",
       "      <td>0.674118</td>\n",
       "      <td>0.956122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.206120</td>\n",
       "      <td>0.676511</td>\n",
       "      <td>0.667302</td>\n",
       "      <td>0.671875</td>\n",
       "      <td>0.955751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.222268</td>\n",
       "      <td>0.689636</td>\n",
       "      <td>0.664852</td>\n",
       "      <td>0.677017</td>\n",
       "      <td>0.956308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.229654</td>\n",
       "      <td>0.685547</td>\n",
       "      <td>0.668935</td>\n",
       "      <td>0.677139</td>\n",
       "      <td>0.956215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.238277</td>\n",
       "      <td>0.690188</td>\n",
       "      <td>0.668391</td>\n",
       "      <td>0.679115</td>\n",
       "      <td>0.956633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/codeGen/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=11580, training_loss=0.10565916324140494, metrics={'train_runtime': 12035.6056, 'train_samples_per_second': 7.696, 'train_steps_per_second': 0.962, 'total_flos': 2.3059619972289792e+17, 'train_loss': 0.10565916324140494, 'epoch': 10.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 8\n",
    "learning_rate = 1e-4\n",
    "\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"codellama-more\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45d34533-33d8-4a6d-93d5-2a6ddcca6620",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"uncodellama.txt\", \"w\") as f:\n",
    "    f.write(f\"{trainer.state.log_history}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51223d61-db1a-4a0d-b357-30b18cfe7db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/codeGen/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "a = trainer.predict(tokenized_ds[\"validation\"])\n",
    "b = trainer.predict(tokenized_ds[\"test\"])\n",
    "c = trainer.predict(tokenized_ds[\"gh\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db86c2ec-2c56-45a9-87f9-22825501af1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ALG': {'f1': 0.1818181818181818,\n",
      "         'number': 9,\n",
      "         'precision': 0.5,\n",
      "         'recall': 0.1111111111111111},\n",
      " 'APP': {'f1': 0.694672131147541,\n",
      "         'number': 480,\n",
      "         'precision': 0.6834677419354839,\n",
      "         'recall': 0.70625},\n",
      " 'CB': {'f1': 0.5161290322580645,\n",
      "        'number': 244,\n",
      "        'precision': 0.5079365079365079,\n",
      "        'recall': 0.5245901639344263},\n",
      " 'CLA': {'f1': 0.655440414507772,\n",
      "         'number': 406,\n",
      "         'precision': 0.6912568306010929,\n",
      "         'recall': 0.6231527093596059},\n",
      " 'DEV': {'f1': 0.7058823529411764,\n",
      "         'number': 149,\n",
      "         'precision': 0.7804878048780488,\n",
      "         'recall': 0.6442953020134228},\n",
      " 'DS': {'f1': 0.7565982404692082,\n",
      "        'number': 177,\n",
      "        'precision': 0.7865853658536586,\n",
      "        'recall': 0.7288135593220338},\n",
      " 'DT': {'f1': 0.7937743190661477,\n",
      "        'number': 134,\n",
      "        'precision': 0.8292682926829268,\n",
      "        'recall': 0.7611940298507462},\n",
      " 'FN': {'f1': 0.7322834645669292,\n",
      "        'number': 134,\n",
      "        'precision': 0.775,\n",
      "        'recall': 0.6940298507462687},\n",
      " 'FT': {'f1': 0.7789473684210526,\n",
      "        'number': 90,\n",
      "        'precision': 0.74,\n",
      "        'recall': 0.8222222222222222},\n",
      " 'FUN': {'f1': 0.689795918367347,\n",
      "         'number': 261,\n",
      "         'precision': 0.7379912663755459,\n",
      "         'recall': 0.6475095785440613},\n",
      " 'HXT': {'f1': 0.7294117647058823,\n",
      "         'number': 38,\n",
      "         'precision': 0.6595744680851063,\n",
      "         'recall': 0.8157894736842105},\n",
      " 'LAN': {'f1': 0.888888888888889,\n",
      "         'number': 154,\n",
      "         'precision': 0.8695652173913043,\n",
      "         'recall': 0.9090909090909091},\n",
      " 'LIB': {'f1': 0.6495726495726496,\n",
      "         'number': 226,\n",
      "         'precision': 0.628099173553719,\n",
      "         'recall': 0.672566371681416},\n",
      " 'OS': {'f1': 0.8888888888888888,\n",
      "        'number': 73,\n",
      "        'precision': 0.9014084507042254,\n",
      "        'recall': 0.8767123287671232},\n",
      " 'UIE': {'f1': 0.6265432098765431,\n",
      "         'number': 330,\n",
      "         'precision': 0.6383647798742138,\n",
      "         'recall': 0.6151515151515151},\n",
      " 'UN': {'f1': 0.6206896551724138,\n",
      "        'number': 30,\n",
      "        'precision': 0.6428571428571429,\n",
      "        'recall': 0.6},\n",
      " 'VAL': {'f1': 0.5884543761638733,\n",
      "         'number': 269,\n",
      "         'precision': 0.5895522388059702,\n",
      "         'recall': 0.587360594795539},\n",
      " 'VAR': {'f1': 0.5569176882661997,\n",
      "         'number': 293,\n",
      "         'precision': 0.5719424460431655,\n",
      "         'recall': 0.5426621160409556},\n",
      " 'VER': {'f1': 0.8913857677902622,\n",
      "         'number': 137,\n",
      "         'precision': 0.9153846153846154,\n",
      "         'recall': 0.8686131386861314},\n",
      " 'WEB': {'f1': 0.6923076923076923,\n",
      "         'number': 39,\n",
      "         'precision': 0.6923076923076923,\n",
      "         'recall': 0.6923076923076923},\n",
      " 'overall_accuracy': 0.9566327122537531,\n",
      " 'overall_f1': 0.6791147994467496,\n",
      " 'overall_precision': 0.6901883609783526,\n",
      " 'overall_recall': 0.6683909610672475}\n",
      "{'ALG': {'f1': 0.72, 'number': 16, 'precision': 1.0, 'recall': 0.5625},\n",
      " 'APP': {'f1': 0.7201946472019465,\n",
      "         'number': 407,\n",
      "         'precision': 0.7132530120481928,\n",
      "         'recall': 0.7272727272727273},\n",
      " 'CB': {'f1': 0.4123711340206186,\n",
      "        'number': 294,\n",
      "        'precision': 0.4166666666666667,\n",
      "        'recall': 0.40816326530612246},\n",
      " 'CLA': {'f1': 0.6666666666666666,\n",
      "         'number': 509,\n",
      "         'precision': 0.6950959488272921,\n",
      "         'recall': 0.6404715127701375},\n",
      " 'DEV': {'f1': 0.7708333333333334,\n",
      "         'number': 53,\n",
      "         'precision': 0.8604651162790697,\n",
      "         'recall': 0.6981132075471698},\n",
      " 'DS': {'f1': 0.7852494577006508,\n",
      "        'number': 244,\n",
      "        'precision': 0.8341013824884793,\n",
      "        'recall': 0.7418032786885246},\n",
      " 'DT': {'f1': 0.8177777777777778,\n",
      "        'number': 111,\n",
      "        'precision': 0.8070175438596491,\n",
      "        'recall': 0.8288288288288288},\n",
      " 'FN': {'f1': 0.7960526315789475,\n",
      "        'number': 163,\n",
      "        'precision': 0.8581560283687943,\n",
      "        'recall': 0.7423312883435583},\n",
      " 'FT': {'f1': 0.7241379310344829,\n",
      "        'number': 127,\n",
      "        'precision': 0.8,\n",
      "        'recall': 0.6614173228346457},\n",
      " 'FUN': {'f1': 0.6506024096385542,\n",
      "         'number': 266,\n",
      "         'precision': 0.6982758620689655,\n",
      "         'recall': 0.6090225563909775},\n",
      " 'HXT': {'f1': 0.4842105263157894,\n",
      "         'number': 52,\n",
      "         'precision': 0.5348837209302325,\n",
      "         'recall': 0.4423076923076923},\n",
      " 'LAN': {'f1': 0.8268156424581005,\n",
      "         'number': 178,\n",
      "         'precision': 0.8222222222222222,\n",
      "         'recall': 0.8314606741573034},\n",
      " 'LIB': {'f1': 0.6459143968871596,\n",
      "         'number': 257,\n",
      "         'precision': 0.6459143968871596,\n",
      "         'recall': 0.6459143968871596},\n",
      " 'OS': {'f1': 0.8955223880597014,\n",
      "        'number': 66,\n",
      "        'precision': 0.8823529411764706,\n",
      "        'recall': 0.9090909090909091},\n",
      " 'UIE': {'f1': 0.592391304347826,\n",
      "         'number': 355,\n",
      "         'precision': 0.5721784776902887,\n",
      "         'recall': 0.6140845070422535},\n",
      " 'UN': {'f1': 0.5283018867924529,\n",
      "        'number': 24,\n",
      "        'precision': 0.4827586206896552,\n",
      "        'recall': 0.5833333333333334},\n",
      " 'VAL': {'f1': 0.47356321839080456,\n",
      "         'number': 213,\n",
      "         'precision': 0.46396396396396394,\n",
      "         'recall': 0.4835680751173709},\n",
      " 'VAR': {'f1': 0.5553914327917282,\n",
      "         'number': 375,\n",
      "         'precision': 0.6225165562913907,\n",
      "         'recall': 0.5013333333333333},\n",
      " 'VER': {'f1': 0.8623853211009174,\n",
      "         'number': 111,\n",
      "         'precision': 0.8785046728971962,\n",
      "         'recall': 0.8468468468468469},\n",
      " 'WEB': {'f1': 0.8169014084507042,\n",
      "         'number': 39,\n",
      "         'precision': 0.90625,\n",
      "         'recall': 0.7435897435897436},\n",
      " 'overall_accuracy': 0.9549414437264286,\n",
      " 'overall_f1': 0.6577056161831248,\n",
      " 'overall_precision': 0.6762452107279694,\n",
      " 'overall_recall': 0.6401554404145078}\n",
      "{'ALG': {'f1': 0.06779661016949153,\n",
      "         'number': 53,\n",
      "         'precision': 0.3333333333333333,\n",
      "         'recall': 0.03773584905660377},\n",
      " 'APP': {'f1': 0.5932489451476792,\n",
      "         'number': 1224,\n",
      "         'precision': 0.6134380453752182,\n",
      "         'recall': 0.5743464052287581},\n",
      " 'CB': {'f1': 0.2614795918367347,\n",
      "        'number': 555,\n",
      "        'precision': 0.20236920039486672,\n",
      "        'recall': 0.36936936936936937},\n",
      " 'CLA': {'f1': 0.4128595600676819,\n",
      "         'number': 274,\n",
      "         'precision': 0.38485804416403785,\n",
      "         'recall': 0.44525547445255476},\n",
      " 'DEV': {'f1': 0.4789644012944984,\n",
      "         'number': 153,\n",
      "         'precision': 0.47435897435897434,\n",
      "         'recall': 0.48366013071895425},\n",
      " 'DS': {'f1': 0.6261980830670927,\n",
      "        'number': 156,\n",
      "        'precision': 0.6242038216560509,\n",
      "        'recall': 0.6282051282051282},\n",
      " 'DT': {'f1': 0.6012269938650306,\n",
      "        'number': 85,\n",
      "        'precision': 0.6282051282051282,\n",
      "        'recall': 0.5764705882352941},\n",
      " 'FN': {'f1': 0.676923076923077,\n",
      "        'number': 227,\n",
      "        'precision': 0.6006825938566553,\n",
      "        'recall': 0.775330396475771},\n",
      " 'FT': {'f1': 0.6623376623376623,\n",
      "        'number': 138,\n",
      "        'precision': 0.6,\n",
      "        'recall': 0.7391304347826086},\n",
      " 'FUN': {'f1': 0.5953177257525084,\n",
      "         'number': 143,\n",
      "         'precision': 0.5705128205128205,\n",
      "         'recall': 0.6223776223776224},\n",
      " 'HXT': {'f1': 0.5999999999999999,\n",
      "         'number': 13,\n",
      "         'precision': 0.5294117647058824,\n",
      "         'recall': 0.6923076923076923},\n",
      " 'LAN': {'f1': 0.8267898383371826,\n",
      "         'number': 209,\n",
      "         'precision': 0.7991071428571429,\n",
      "         'recall': 0.8564593301435407},\n",
      " 'LIB': {'f1': 0.4386113463166808,\n",
      "         'number': 579,\n",
      "         'precision': 0.43023255813953487,\n",
      "         'recall': 0.4473229706390328},\n",
      " 'OS': {'f1': 0.8660968660968661,\n",
      "        'number': 175,\n",
      "        'precision': 0.8636363636363636,\n",
      "        'recall': 0.8685714285714285},\n",
      " 'UIE': {'f1': 0.5695364238410596,\n",
      "         'number': 282,\n",
      "         'precision': 0.5341614906832298,\n",
      "         'recall': 0.6099290780141844},\n",
      " 'UN': {'f1': 0.5545023696682464,\n",
      "        'number': 182,\n",
      "        'precision': 0.4875,\n",
      "        'recall': 0.6428571428571429},\n",
      " 'VAL': {'f1': 0.0, 'number': 0, 'precision': 0.0, 'recall': 0.0},\n",
      " 'VAR': {'f1': 0.14921630094043886,\n",
      "         'number': 348,\n",
      "         'precision': 0.0954290296712109,\n",
      "         'recall': 0.34195402298850575},\n",
      " 'VER': {'f1': 0.7547770700636943,\n",
      "         'number': 333,\n",
      "         'precision': 0.8033898305084746,\n",
      "         'recall': 0.7117117117117117},\n",
      " 'WEB': {'f1': 0.5737051792828685,\n",
      "         'number': 145,\n",
      "         'precision': 0.6792452830188679,\n",
      "         'recall': 0.496551724137931},\n",
      " 'overall_accuracy': 0.9126723750663636,\n",
      " 'overall_f1': 0.4711546176682982,\n",
      " 'overall_precision': 0.40840172485742104,\n",
      " 'overall_recall': 0.5566932119833143}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pprint import pprint\n",
    "\n",
    "def get_dct(x, text):\n",
    "    pred = np.argmax(x.predictions, axis=2)\n",
    "    dct = compute_metrics((pred, tokenized_ds[text][\"labels\"]), True)\n",
    "    with open(f'uncodellama_{text}.pickle', 'wb') as f:\n",
    "        pickle.dump(dct, f)\n",
    "\n",
    "    return dct\n",
    "\n",
    "pprint(get_dct(a, \"validation\"))\n",
    "pprint(get_dct(b, \"test\"))\n",
    "pprint(get_dct(c, \"gh\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13897882-68cf-440c-807e-f71fbd7aa7fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codeGen",
   "language": "python",
   "name": "codegen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
